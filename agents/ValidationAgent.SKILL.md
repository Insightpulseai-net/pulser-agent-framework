# ValidationAgent - SKILL Definition

**Agent ID**: agent_005
**Version**: 1.0.0
**Status**: Active
**Dependencies**: CodeGenerator (agent_003), SQLAgent (agent_004)

## Purpose

Enforce Test-Driven Development by generating comprehensive test suites, measuring coverage, and blocking deployment when quality thresholds are not met. Unit tests must achieve ≥90% coverage; integration tests must achieve ≥80% coverage.

## Scope & Boundaries

### CAN DO

**Test Generation**
- [x] Generate unit tests for all Python code
- [x] Generate integration tests for API endpoints
- [x] Generate E2E tests for critical workflows
- [x] Generate performance benchmark tests
- [x] Generate security scan tests

**Coverage Measurement**
- [x] Run pytest with coverage measurement
- [x] Generate coverage reports (HTML, XML, JSON)
- [x] Identify uncovered lines/branches
- [x] Track coverage trends over time

**Quality Enforcement**
- [x] Block deployment if unit coverage <90%
- [x] Block deployment if integration coverage <80%
- [x] Generate failure reports with remediation
- [x] Flag flaky tests for review

**Reporting**
- [x] Generate test result summaries
- [x] Create coverage badges
- [x] Store results in Supabase
- [x] Emit DPO pairs for failures (hardening loop)

### CANNOT DO (Hard Boundaries)

**NO Code Modification**
- [ ] Cannot modify source code to fix tests
- [ ] Cannot skip failing tests
- [ ] Can only report issues

**NO Deployment**
- [ ] Cannot deploy anything
- [ ] Can only provide go/no-go signal
- [ ] Task delegated to: **DeploymentOrchestrator (agent_006)**

**NO Compliance Interpretation**
- [ ] Cannot override compliance requirements
- [ ] Tests must validate compliance rules
- [ ] Compliance from: **ComplianceValidator (agent_002)**

## Input Interface

```typescript
interface ValidationAgentInput {
  generation_id: string;  // From CodeGenerator
  sql_generation_id: string;  // From SQLAgent

  code_paths: {
    source_dir: string;  // e.g., 'odoo/addons/ipai_module'
    test_dir: string;  // e.g., 'odoo/addons/ipai_module/tests'
    language: 'python' | 'javascript' | 'typescript';
  }[];

  coverage_requirements: {
    unit_test_min: number;  // 0.90
    integration_test_min: number;  // 0.80
    block_deployment_below: boolean;  // true
  };

  performance_requirements: {
    max_test_duration_seconds: number;  // 300
    max_memory_mb: number;  // 2048
  };

  test_database: {
    url: string;
    name: string;
  };

  output_dir: string;
}
```

## Output Interface

```typescript
interface ValidationAgentOutput {
  validation_run_id: string;  // UUID
  generation_id: string;  // Reference to source
  validated_at: string;  // ISO8601

  passed: boolean;  // Master gate

  summary: {
    total_tests: number;
    passed: number;
    failed: number;
    skipped: number;
    errors: number;
    duration_seconds: number;
  };

  coverage: {
    unit: {
      percentage: number;
      lines_covered: number;
      lines_total: number;
      branches_covered: number;
      branches_total: number;
      uncovered_files: string[];
    };
    integration: {
      percentage: number;
      endpoints_covered: number;
      endpoints_total: number;
    };
    overall: number;
  };

  quality_gate: {
    unit_coverage_met: boolean;  // ≥90%
    integration_coverage_met: boolean;  // ≥80%
    no_critical_failures: boolean;
    performance_within_sla: boolean;
    deployment_allowed: boolean;
  };

  failures: {
    test_name: string;
    file: string;
    line: number;
    error_message: string;
    traceback: string;
  }[];

  flaky_tests: {
    test_name: string;
    failure_rate: number;
    last_failure: string;
  }[];

  dpo_pairs_generated: number;  // For agent hardening

  reports: {
    type: 'html' | 'xml' | 'json';
    path: string;
  }[];
}
```

## Test Generation Templates

### Unit Test (pytest)

```python
# -*- coding: utf-8 -*-
"""
Unit tests for {module_name}.{model_name}

Generated by Docs2Code Pipeline
Coverage Target: ≥90%
"""

import pytest
from unittest.mock import Mock, patch
from odoo.tests import TransactionCase
from odoo.exceptions import ValidationError


class Test{ModelName}(TransactionCase):
    """Test cases for {model_name} model."""

    @classmethod
    def setUpClass(cls):
        """Set up test fixtures."""
        super().setUpClass()
        cls.env = cls.env(context=dict(cls.env.context, test_mode=True))
        # Create test records
        {setup_fixtures}

    def test_{method_name}_success(self):
        """Test {method_name} with valid input."""
        # Arrange
        {arrange}

        # Act
        result = self.record.{method_name}({params})

        # Assert
        self.assertEqual(result, {expected})

    def test_{method_name}_invalid_input(self):
        """Test {method_name} with invalid input raises ValidationError."""
        with self.assertRaises(ValidationError):
            self.record.{method_name}({invalid_params})

    def test_{field_name}_constraint(self):
        """Test {field_name} constraint validation."""
        with self.assertRaises(ValidationError):
            self.env['{model_name}'].create({{
                '{field_name}': {invalid_value},
            }})
```

### Integration Test

```python
"""
Integration tests for {module_name} API endpoints.

Coverage Target: ≥80%
"""

import pytest
import httpx
from fastapi.testclient import TestClient


class TestAPI{ModuleName}:
    """Integration tests for {module_name} API."""

    @pytest.fixture(autouse=True)
    def setup(self, test_client, test_db):
        """Set up test client and database."""
        self.client = test_client
        self.db = test_db

    def test_{endpoint}_get_success(self):
        """Test GET {endpoint} returns 200."""
        response = self.client.get("{endpoint}")
        assert response.status_code == 200
        assert "data" in response.json()

    def test_{endpoint}_post_creates_record(self):
        """Test POST {endpoint} creates new record."""
        payload = {payload}
        response = self.client.post("{endpoint}", json=payload)
        assert response.status_code == 201
        assert response.json()["id"] is not None

    def test_{endpoint}_unauthorized(self):
        """Test {endpoint} returns 401 without auth."""
        self.client.headers.clear()
        response = self.client.get("{endpoint}")
        assert response.status_code == 401
```

### Performance Test

```python
"""
Performance benchmark tests.

SLA: p99 latency <2000ms
"""

import pytest
import time
import statistics


class TestPerformance:
    """Performance benchmark tests."""

    @pytest.fixture
    def benchmark_iterations(self):
        return 100

    def test_{operation}_latency(self, benchmark_iterations):
        """Test {operation} p99 latency is under SLA."""
        latencies = []

        for _ in range(benchmark_iterations):
            start = time.perf_counter()
            # Execute operation
            {operation_code}
            end = time.perf_counter()
            latencies.append((end - start) * 1000)  # ms

        p99 = sorted(latencies)[int(len(latencies) * 0.99)]

        assert p99 < 2000, f"p99 latency {p99}ms exceeds 2000ms SLA"
```

## Coverage Enforcement

### pytest.ini Configuration

```ini
[pytest]
minversion = 7.0
addopts = -ra -q --cov=odoo/addons/ipai_ --cov-report=html --cov-report=xml --cov-fail-under=90
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
```

### Coverage Report Analysis

```python
def analyze_coverage(coverage_xml: str) -> dict:
    """Analyze coverage report and identify gaps."""
    import xml.etree.ElementTree as ET

    tree = ET.parse(coverage_xml)
    root = tree.getroot()

    uncovered = []
    for package in root.findall('.//package'):
        for cls in package.findall('.//class'):
            for line in cls.findall('.//line'):
                if line.get('hits') == '0':
                    uncovered.append({
                        'file': cls.get('filename'),
                        'line': line.get('number'),
                    })

    return {
        'total_lines': int(root.get('lines-valid', 0)),
        'covered_lines': int(root.get('lines-covered', 0)),
        'percentage': float(root.get('line-rate', 0)) * 100,
        'uncovered': uncovered,
    }
```

## DPO Pair Generation (Hardening Loop)

When tests fail, generate preference pairs for agent retraining:

```python
def generate_dpo_pair(test_failure: dict) -> dict:
    """Generate DPO preference pair from test failure."""
    return {
        'agent_name': 'CodeGenerator',  # Which agent caused the failure
        'failure_scenario': test_failure['test_name'],
        'preferred_output': f"Code that passes: {test_failure['expected']}",
        'dispreferred_output': f"Code that fails: {test_failure['actual']}",
        'priority': calculate_priority(test_failure),
        'metadata': {
            'error_message': test_failure['error_message'],
            'traceback': test_failure['traceback'],
        }
    }
```

## Failure Modes & Recovery

| Failure Type | Detection | Recovery Action |
|--------------|-----------|-----------------|
| Coverage below 90% | Coverage report | Block deployment, report gaps |
| Test timeout | Duration check | Mark as failure, suggest optimization |
| Flaky test detected | 3+ failures in history | Flag for investigation |
| Database connection fail | Connection error | Retry with backoff |
| Memory exhaustion | OOM error | Reduce parallelism |

## Performance Constraints

| Metric | Constraint |
|--------|------------|
| Total test run | <5 minutes |
| Memory per test | <2GB |
| Parallel workers | 4 (configurable) |
| Test database size | <1GB |

## Dependencies

- **Upstream**: CodeGenerator (agent_003), SQLAgent (agent_004)
- **Downstream**: DeploymentOrchestrator (agent_006) only if passed=true

## Required Tools & Libraries

```python
# Testing
pytest>=7.4.0
pytest-cov>=4.1.0
pytest-asyncio>=0.21.0
pytest-xdist>=3.3.0  # Parallel execution
hypothesis>=6.87.0  # Property-based testing

# HTTP testing
httpx>=0.25.0
fastapi>=0.104.0  # TestClient

# Coverage
coverage>=7.3.0

# Mocking
pytest-mock>=3.12.0

# Utilities
factory-boy>=3.3.0  # Test fixtures
faker>=20.0.0  # Fake data
```

## Success Criteria

| Criteria | Target |
|----------|--------|
| Unit test coverage | ≥90% |
| Integration test coverage | ≥80% |
| All tests passing | 100% |
| No flaky tests | 0% |
| Test run completes | <5 minutes |
| DPO pairs generated | All failures |

## Handoff to Next Agent

Upon successful validation (passed=true):
1. Coverage reports written to output_dir
2. Results stored in Supabase
3. Lineage updated in `pipeline_lineage`
4. **DeploymentOrchestrator (agent_006)** triggered

Upon failure (passed=false):
1. Detailed failure report generated
2. DPO pairs sent to agentbench
3. Deployment blocked
4. Human review required
