# .github/workflows/docs2code.yml
# Docs2Code Pipeline - Compile, Generate, Verify
#
# Based on GPT-5 prompting research and Databricks agentic debugging patterns.
# Implements deterministic DocIR compilation, patch-mode generation, and
# replay-based evaluation.

name: Docs2Code Pipeline

on:
  push:
    branches: [main]
    paths:
      - 'docs/sources/**'
      - 'tools/**'
      - 'pipelines/**'
      - 'eval/**'
      - '.github/workflows/docs2code.yml'
  pull_request:
    paths:
      - 'docs/sources/**'
      - 'tools/**'
      - 'pipelines/**'
      - 'eval/**'
  workflow_dispatch:
    inputs:
      skip_eval:
        description: 'Skip evaluation suite'
        required: false
        default: 'false'
        type: boolean
      apply_patches:
        description: 'Apply generated patches (caution: modifies code)'
        required: false
        default: 'false'
        type: boolean

concurrency:
  group: docs2code-${{ github.ref }}
  cancel-in-progress: true

env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '20'

jobs:
  # ============================================================================
  # Stage 1: Compile DocIR
  # ============================================================================
  compile:
    name: Compile DocIR
    runs-on: ubuntu-latest
    outputs:
      docir_hash: ${{ steps.compile.outputs.docir_hash }}
      requirement_count: ${{ steps.compile.outputs.requirement_count }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          pip install pyyaml jsonschema

      - name: Create source directories
        run: |
          mkdir -p docs/sources docs/docir

      - name: Compile DocIR
        id: compile
        run: |
          # Compile documentation sources to DocIR
          python tools/compile_docir.py \
            --in docs/sources \
            --out docs/docir/docir.json \
            --doc-id "${{ github.repository }}" \
            --validate || echo "Compilation completed with warnings"

          # Extract stats for outputs
          if [ -f docs/docir/docir.json ]; then
            DOCIR_HASH=$(sha256sum docs/docir/docir.json | cut -d' ' -f1 | head -c 16)
            REQ_COUNT=$(python -c "import json; d=json.load(open('docs/docir/docir.json')); print(len(d.get('requirements', [])))")
            echo "docir_hash=$DOCIR_HASH" >> $GITHUB_OUTPUT
            echo "requirement_count=$REQ_COUNT" >> $GITHUB_OUTPUT
            echo "::notice::DocIR compiled with $REQ_COUNT requirements (hash: $DOCIR_HASH)"
          else
            echo "docir_hash=empty" >> $GITHUB_OUTPUT
            echo "requirement_count=0" >> $GITHUB_OUTPUT
          fi

      - name: Validate DocIR Schema
        run: |
          if [ -f docs/docir/docir.json ]; then
            python tools/validate_jsonschema.py \
              docs/docir/docir.json \
              tools/schemas/DocIR.schema.json || true
          fi

      - name: Upload DocIR artifact
        uses: actions/upload-artifact@v4
        with:
          name: docir
          path: docs/docir/
          retention-days: 7

  # ============================================================================
  # Stage 2: Generate (Patch Mode)
  # ============================================================================
  generate:
    name: Generate Code
    runs-on: ubuntu-latest
    needs: compile
    if: needs.compile.outputs.requirement_count != '0'

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Download DocIR
        uses: actions/download-artifact@v4
        with:
          name: docir
          path: docs/docir/

      - name: Install dependencies
        run: |
          pip install pyyaml

      - name: Generate patches
        id: generate
        run: |
          mkdir -p patches

          # Generate code in patch mode
          python tools/patch_generator.py \
            --docir docs/docir/docir.json \
            --output patches/ \
            --target . \
            --verbose

          # Count changes
          PATCH_FILES=$(ls -1 patches/*.patch 2>/dev/null | wc -l)
          echo "patch_count=$PATCH_FILES" >> $GITHUB_OUTPUT
          echo "::notice::Generated $PATCH_FILES patch files"

      - name: Apply patches (if requested)
        if: github.event.inputs.apply_patches == 'true'
        run: |
          python tools/patch_generator.py \
            --docir docs/docir/docir.json \
            --target . \
            --apply

      - name: Upload patches
        uses: actions/upload-artifact@v4
        with:
          name: patches
          path: patches/
          retention-days: 7

  # ============================================================================
  # Stage 3: Verify
  # ============================================================================
  verify:
    name: Verify Generated Code
    runs-on: ubuntu-latest
    needs: [compile, generate]

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Download artifacts
        uses: actions/download-artifact@v4

      - name: Install dependencies
        run: |
          pip install black isort mypy pylint pyyaml
          pip install -r requirements.txt || true
          pip install -r requirements-dev.txt || true

      - name: Lint check
        run: |
          # Check Python files in generated output
          if [ -d "addons" ]; then
            black --check addons/ || echo "Black formatting issues found"
            isort --check-only addons/ || echo "Import sorting issues found"
          fi

      - name: Type check
        run: |
          if [ -d "addons" ]; then
            mypy addons/ --ignore-missing-imports || echo "Type check completed with issues"
          fi

      - name: Unit tests
        run: |
          if [ -d "tests" ]; then
            pytest tests/unit/ -v --tb=short || echo "Some unit tests failed"
          fi

  # ============================================================================
  # Stage 4: Replay Evaluation
  # ============================================================================
  eval:
    name: Replay Evaluation
    runs-on: ubuntu-latest
    needs: verify
    if: github.event.inputs.skip_eval != 'true'

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Download artifacts
        uses: actions/download-artifact@v4

      - name: Install dependencies
        run: |
          pip install pyyaml

      - name: Run replay evaluation
        id: eval
        run: |
          mkdir -p eval/reports

          python eval/run_replay.py \
            --cases eval/cases \
            --thresholds eval/thresholds.yaml \
            --registry tools/registry.yaml \
            --output eval/reports/report.json \
            --verbose || EVAL_FAILED=1

          if [ "$EVAL_FAILED" = "1" ]; then
            echo "::warning::Some evaluation cases failed"
            echo "eval_passed=false" >> $GITHUB_OUTPUT
          else
            echo "eval_passed=true" >> $GITHUB_OUTPUT
          fi

      - name: Upload evaluation report
        uses: actions/upload-artifact@v4
        with:
          name: eval-report
          path: eval/reports/
          retention-days: 30

      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            let report = {};
            try {
              report = JSON.parse(fs.readFileSync('eval/reports/report.json', 'utf8'));
            } catch (e) {
              console.log('No report file found');
              return;
            }

            const passRate = (report.summary?.pass_rate * 100).toFixed(1);
            const status = report.cases_failed === 0 ? '✅' : '⚠️';

            const body = `## Docs2Code Evaluation Report ${status}

            | Metric | Value | Threshold |
            |--------|-------|-----------|
            | Pass Rate | ${passRate}% | 90% |
            | Accuracy | ${(report.summary?.avg_accuracy * 100).toFixed(1)}% | 85% |
            | Helpfulness | ${(report.summary?.avg_helpfulness * 100).toFixed(1)}% | 80% |
            | Actionability | ${(report.summary?.avg_actionability * 100).toFixed(1)}% | 75% |
            | Safety | ${(report.summary?.avg_safety * 100).toFixed(1)}% | 100% |

            **Cases:** ${report.cases_total} total, ${report.cases_passed} passed, ${report.cases_failed} failed
            `;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: body
            });

  # ============================================================================
  # Stage 5: Report & Summary
  # ============================================================================
  summary:
    name: Pipeline Summary
    runs-on: ubuntu-latest
    needs: [compile, generate, verify, eval]
    if: always()

    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@v4
        continue-on-error: true

      - name: Generate summary
        run: |
          echo "## Docs2Code Pipeline Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Stage | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Compile | ${{ needs.compile.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Generate | ${{ needs.generate.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Verify | ${{ needs.verify.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Eval | ${{ needs.eval.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**DocIR Hash:** ${{ needs.compile.outputs.docir_hash }}" >> $GITHUB_STEP_SUMMARY
          echo "**Requirements:** ${{ needs.compile.outputs.requirement_count }}" >> $GITHUB_STEP_SUMMARY
