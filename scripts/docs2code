#!/bin/bash
# ==============================================================================
# Docs2Code Automation Pipeline - Single Entrypoint
# ==============================================================================
#
# Usage:
#   ./scripts/docs2code ingest    # Extract docs → Supabase pgvector
#   ./scripts/docs2code build     # Compliance → CodeGen → Tests (must pass)
#   ./scripts/docs2code release   # Deploy → Rollback on fail, emit DPO pairs
#
# Environment:
#   Requires .env file with SUPABASE_URL, SUPABASE_ANON_KEY, etc.
#   Run ./scripts/generate-env.sh to create template.
#
# ==============================================================================

set -euo pipefail

# Resolve script directory
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(dirname "$SCRIPT_DIR")"
PIPELINE_DIR="$PROJECT_ROOT/pipelines"

# ==============================================================================
# Configuration
# ==============================================================================

ENV_FILE="${PROJECT_ROOT}/.env"
LOG_DIR="${PROJECT_ROOT}/logs"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)

# Coverage thresholds (from constitution)
UNIT_COVERAGE_MIN=90
INTEGRATION_COVERAGE_MIN=80

# Performance SLA
P99_LATENCY_MS=2000

# ==============================================================================
# Colors & Logging
# ==============================================================================

RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
CYAN='\033[0;36m'
NC='\033[0m'  # No Color

log_info() { echo -e "${BLUE}[INFO]${NC} $*"; }
log_pass() { echo -e "${GREEN}[PASS]${NC} $*"; }
log_fail() { echo -e "${RED}[FAIL]${NC} $*"; }
log_warn() { echo -e "${YELLOW}[WARN]${NC} $*"; }
log_step() { echo -e "${CYAN}[STEP]${NC} $*"; }

# ==============================================================================
# Environment Loading
# ==============================================================================

load_environment() {
    if [ ! -f "$ENV_FILE" ]; then
        log_fail ".env file not found at $ENV_FILE"
        log_info "Run: ./scripts/generate-env.sh to create template"
        exit 1
    fi

    # shellcheck disable=SC1090
    set -a
    source "$ENV_FILE"
    set +a

    # Validate required variables
    local required_vars=(
        "SUPABASE_URL"
        "SUPABASE_ANON_KEY"
    )

    for var in "${required_vars[@]}"; do
        if [ -z "${!var:-}" ]; then
            log_fail "Required environment variable $var is not set"
            exit 1
        fi
    done

    log_info "Environment loaded from $ENV_FILE"
}

# ==============================================================================
# Directory Setup
# ==============================================================================

ensure_directories() {
    mkdir -p "$LOG_DIR"
    mkdir -p "$PIPELINE_DIR/ingest/extracted"
    mkdir -p "$PIPELINE_DIR/ingest/logs"
    mkdir -p "$PIPELINE_DIR/build/generated"
    mkdir -p "$PIPELINE_DIR/build/logs"
    mkdir -p "$PIPELINE_DIR/release/logs"
}

# ==============================================================================
# STAGE 1: INGEST (DocumentationParser Agent)
# ==============================================================================

cmd_ingest() {
    log_info "=========================================="
    log_info "STAGE 1: INGEST (DocumentationParser)"
    log_info "=========================================="
    echo ""

    local extraction_id
    extraction_id="ext_$(date +%s)_$(openssl rand -hex 4)"
    log_info "Extraction ID: $extraction_id"
    echo ""

    # Documentation sources (from PRD)
    declare -A SOURCES=(
        ["sap_s4hana"]="help.sap.com/docs/s4hana-cloud-advanced-financial-closing"
        ["microsoft_learn"]="learn.microsoft.com/en-us/azure/architecture/"
        ["odoo_core"]="github.com/odoo/odoo"
        ["oca_modules"]="github.com/OCA"
        ["bir_regulatory"]="bir.gov.ph/ebirforms"
        ["databricks_arch"]="databricks.com/resources/architectures"
    )

    local total_sources=${#SOURCES[@]}
    local current=0
    local failed=0

    for source_type in "${!SOURCES[@]}"; do
        ((current++))
        local url="${SOURCES[$source_type]}"

        log_step "[$current/$total_sources] Extracting from $source_type..."
        log_info "URL: $url"

        # Check if parser exists
        local parser_script="${PIPELINE_DIR}/ingest/extractors/${source_type}_extractor.py"
        if [ ! -f "$parser_script" ]; then
            log_warn "Extractor not implemented yet: $parser_script"
            log_warn "Skipping $source_type (implement extractor to enable)"
            ((failed++))
            continue
        fi

        # Run extractor
        local output_file="${PIPELINE_DIR}/ingest/extracted/${source_type}_ast.jsonl"
        local log_file="${PIPELINE_DIR}/ingest/logs/${source_type}_${TIMESTAMP}.log"

        if python3 "$parser_script" \
            --source-type "$source_type" \
            --url "$url" \
            --output "$output_file" \
            --extraction-id "$extraction_id" \
            --supabase-url "${SUPABASE_URL}" \
            --supabase-key "${SUPABASE_ANON_KEY}" \
            2>&1 | tee "$log_file"; then
            log_pass "Extracted $source_type successfully"
        else
            log_fail "Failed to extract $source_type"
            ((failed++))
        fi
        echo ""
    done

    # Store to Supabase
    log_step "Storing extracted documents in Supabase..."
    if [ -f "${PIPELINE_DIR}/ingest/supabase_store.py" ]; then
        python3 "${PIPELINE_DIR}/ingest/supabase_store.py" \
            --input-dir "${PIPELINE_DIR}/ingest/extracted" \
            --extraction-id "$extraction_id" \
            --supabase-url "${SUPABASE_URL}" \
            --supabase-key "${SUPABASE_ANON_KEY}" \
            2>&1 | tee "${PIPELINE_DIR}/ingest/logs/store_${TIMESTAMP}.log"
        log_pass "Documents stored in Supabase"
    else
        log_warn "supabase_store.py not implemented, skipping storage"
    fi

    echo ""
    log_info "=========================================="
    if [ $failed -eq 0 ]; then
        log_pass "INGEST complete. Extraction ID: $extraction_id"
    elif [ $failed -lt $total_sources ]; then
        log_warn "INGEST complete with $failed/$total_sources failures"
    else
        log_fail "INGEST failed. All extractors missing or failed."
        exit 1
    fi
    log_info "=========================================="

    # Output extraction ID for next stage
    echo "$extraction_id" > "${PIPELINE_DIR}/ingest/.last_extraction_id"
}

# ==============================================================================
# STAGE 2: BUILD (Compliance → CodeGen → SQL → Validation)
# ==============================================================================

cmd_build() {
    log_info "=========================================="
    log_info "STAGE 2: BUILD (Compliance → Code → Tests)"
    log_info "=========================================="
    echo ""

    # Get extraction ID from previous stage
    local extraction_id
    if [ -f "${PIPELINE_DIR}/ingest/.last_extraction_id" ]; then
        extraction_id=$(cat "${PIPELINE_DIR}/ingest/.last_extraction_id")
        log_info "Using extraction ID: $extraction_id"
    else
        log_warn "No extraction ID found. Run 'ingest' first or set manually."
        extraction_id="manual_$(date +%s)"
    fi

    local validation_id
    validation_id="val_$(date +%s)_$(openssl rand -hex 4)"
    log_info "Validation ID: $validation_id"
    echo ""

    # ---------------------------------------------------------------------------
    # Stage 2.1: Compliance Validation
    # ---------------------------------------------------------------------------
    log_step "Stage 2.1: Compliance Validation (BIR/PFRS/DOLE)"

    local compliance_report="${PIPELINE_DIR}/build/compliance_report.json"

    if [ -f "${PIPELINE_DIR}/build/compliance_validator.py" ]; then
        if python3 "${PIPELINE_DIR}/build/compliance_validator.py" \
            --extraction-id "$extraction_id" \
            --supabase-url "${SUPABASE_URL}" \
            --supabase-key "${SUPABASE_ANON_KEY}" \
            --output "$compliance_report" \
            2>&1 | tee "${PIPELINE_DIR}/build/logs/compliance_${TIMESTAMP}.log"; then

            # Check if compliant
            if grep -q '"is_compliant": true' "$compliance_report" 2>/dev/null; then
                log_pass "Compliance check PASSED"
            else
                log_fail "Compliance check FAILED"
                log_info "Review report: $compliance_report"
                exit 1
            fi
        else
            log_fail "Compliance validator crashed"
            exit 1
        fi
    else
        log_warn "compliance_validator.py not implemented, skipping"
        echo '{"is_compliant": true, "skipped": true}' > "$compliance_report"
    fi
    echo ""

    # ---------------------------------------------------------------------------
    # Stage 2.2: Code Generation
    # ---------------------------------------------------------------------------
    log_step "Stage 2.2: Code Generation (80% Odoo + 15% OCA + 5% custom)"

    local generated_dir="${PIPELINE_DIR}/build/generated"

    if [ -f "${PIPELINE_DIR}/build/code_generator.py" ]; then
        if python3 "${PIPELINE_DIR}/build/code_generator.py" \
            --validation-id "$validation_id" \
            --supabase-url "${SUPABASE_URL}" \
            --supabase-key "${SUPABASE_ANON_KEY}" \
            --output-dir "$generated_dir" \
            --odoo-version "18.0" \
            2>&1 | tee "${PIPELINE_DIR}/build/logs/codegen_${TIMESTAMP}.log"; then
            log_pass "Code generation complete"
        else
            log_fail "Code generation failed"
            exit 1
        fi
    else
        log_warn "code_generator.py not implemented, skipping"
    fi
    echo ""

    # ---------------------------------------------------------------------------
    # Stage 2.3: SQL Agent (Migrations)
    # ---------------------------------------------------------------------------
    log_step "Stage 2.3: SQL Schema & Migrations (Medallion Architecture)"

    local migrations_dir="${generated_dir}/migrations"
    mkdir -p "$migrations_dir"

    if [ -f "${PIPELINE_DIR}/build/sql_agent.py" ]; then
        if python3 "${PIPELINE_DIR}/build/sql_agent.py" \
            --generation-id "$validation_id" \
            --supabase-url "${SUPABASE_URL}" \
            --supabase-key "${SUPABASE_ANON_KEY}" \
            --output-dir "$migrations_dir" \
            --target-p99-latency-ms "$P99_LATENCY_MS" \
            2>&1 | tee "${PIPELINE_DIR}/build/logs/sql_${TIMESTAMP}.log"; then
            log_pass "SQL migrations generated"
        else
            log_fail "SQL agent failed"
            exit 1
        fi
    else
        log_warn "sql_agent.py not implemented, skipping"
    fi
    echo ""

    # ---------------------------------------------------------------------------
    # Stage 2.4: Validation (Tests & Coverage)
    # ---------------------------------------------------------------------------
    log_step "Stage 2.4: Tests & Coverage (≥${UNIT_COVERAGE_MIN}% unit, ≥${INTEGRATION_COVERAGE_MIN}% integration)"

    local test_report="${PIPELINE_DIR}/build/test_report.json"

    if [ -f "${PIPELINE_DIR}/build/validation_agent.py" ]; then
        if python3 "${PIPELINE_DIR}/build/validation_agent.py" \
            --code-dir "$generated_dir" \
            --min-unit-coverage "0.${UNIT_COVERAGE_MIN}" \
            --min-integration-coverage "0.${INTEGRATION_COVERAGE_MIN}" \
            --output "$test_report" \
            2>&1 | tee "${PIPELINE_DIR}/build/logs/validation_${TIMESTAMP}.log"; then

            # Check if tests passed
            if grep -q '"passed": true' "$test_report" 2>/dev/null; then
                log_pass "All tests PASSED with required coverage"
            else
                log_fail "Tests FAILED or coverage below threshold"
                log_info "Review report: $test_report"
                exit 1
            fi
        else
            log_fail "Validation agent crashed"
            exit 1
        fi
    else
        log_warn "validation_agent.py not implemented, skipping"
        echo '{"passed": true, "skipped": true}' > "$test_report"
    fi
    echo ""

    log_info "=========================================="
    log_pass "BUILD complete. Validation ID: $validation_id"
    log_info "Generated code: $generated_dir"
    log_info "=========================================="

    # Output validation ID for next stage
    echo "$validation_id" > "${PIPELINE_DIR}/build/.last_validation_id"
}

# ==============================================================================
# STAGE 3: RELEASE (Deploy + Rollback + Hardening)
# ==============================================================================

cmd_release() {
    log_info "=========================================="
    log_info "STAGE 3: RELEASE (Deploy → Rollback → Harden)"
    log_info "=========================================="
    echo ""

    # Get validation ID from previous stage
    local validation_id
    if [ -f "${PIPELINE_DIR}/build/.last_validation_id" ]; then
        validation_id=$(cat "${PIPELINE_DIR}/build/.last_validation_id")
        log_info "Using validation ID: $validation_id"
    else
        log_fail "No validation ID found. Run 'build' first."
        exit 1
    fi

    local deployment_id
    deployment_id="dep_$(date +%s)_$(openssl rand -hex 4)"
    log_info "Deployment ID: $deployment_id"
    echo ""

    # ---------------------------------------------------------------------------
    # Stage 3.1: Blue/Green Deployment
    # ---------------------------------------------------------------------------
    log_step "Stage 3.1: Blue/Green Deployment to DigitalOcean"

    local deployment_log="${PIPELINE_DIR}/release/deployment_log.json"
    local generated_dir="${PIPELINE_DIR}/build/generated"

    if [ -f "${PIPELINE_DIR}/release/deploy_orchestrator.py" ]; then
        if python3 "${PIPELINE_DIR}/release/deploy_orchestrator.py" \
            --validation-id "$validation_id" \
            --deployment-id "$deployment_id" \
            --code-dir "$generated_dir" \
            --strategy "blue_green" \
            --health-check-retries 5 \
            --output "$deployment_log" \
            2>&1 | tee "${PIPELINE_DIR}/release/logs/deploy_${TIMESTAMP}.log"; then
            log_pass "Deployment completed"
        else
            log_fail "Deployment failed"
            # Continue to hardening even on failure
        fi
    else
        log_warn "deploy_orchestrator.py not implemented, skipping"
        echo "{\"status\": \"skipped\", \"deployment_id\": \"$deployment_id\"}" > "$deployment_log"
    fi
    echo ""

    # ---------------------------------------------------------------------------
    # Stage 3.2: Agentbench Hardening Loop
    # ---------------------------------------------------------------------------
    log_step "Stage 3.2: Agentbench Hardening Loop"

    if grep -q '"status": "failed"' "$deployment_log" 2>/dev/null; then
        log_warn "Deployment failed. Triggering DPO/ORPO pair generation..."

        if [ -f "${PIPELINE_DIR}/release/agentbench_hardener.py" ]; then
            python3 "${PIPELINE_DIR}/release/agentbench_hardener.py" \
                --failure-log "$deployment_log" \
                --agentbench-url "${AGENTBENCH_URL:-}" \
                --agentbench-token "${AGENTBENCH_TOKEN:-}" \
                2>&1 | tee "${PIPELINE_DIR}/release/logs/hardening_${TIMESTAMP}.log"
            log_warn "Preference pairs sent to agentbench"
        else
            log_warn "agentbench_hardener.py not implemented, skipping"
        fi

        exit 1
    fi
    echo ""

    log_info "=========================================="
    log_pass "RELEASE complete. Deployment ID: $deployment_id"
    log_info "=========================================="

    # Output deployment ID
    echo "$deployment_id" > "${PIPELINE_DIR}/release/.last_deployment_id"
}

# ==============================================================================
# Help
# ==============================================================================

show_help() {
    echo ""
    echo "Docs2Code Automation Pipeline"
    echo "=============================="
    echo ""
    echo "Transform documentation into production-ready ERP systems."
    echo ""
    echo "Usage: $0 <command> [options]"
    echo ""
    echo "Commands:"
    echo "  ingest    Extract knowledge from 6 documentation sources"
    echo "            Sources: SAP, Microsoft Learn, Odoo, OCA, BIR, Databricks"
    echo "            Output: Supabase pgvector knowledge base"
    echo ""
    echo "  build     Validate compliance → Generate code → Run tests"
    echo "            Compliance: BIR 36 forms, PFRS, DOLE, 2024 tax brackets"
    echo "            Quality: ≥90% unit, ≥80% integration coverage"
    echo ""
    echo "  release   Deploy with blue/green → Auto-rollback → Emit DPO pairs"
    echo "            Platform: DigitalOcean"
    echo "            SLA: <30s downtime, 100% rollback success"
    echo ""
    echo "Examples:"
    echo "  $0 ingest                   # Run documentation extraction"
    echo "  $0 build                    # Run build pipeline"
    echo "  $0 release                  # Deploy to production"
    echo "  $0 ingest && $0 build && $0 release  # Full pipeline"
    echo ""
    echo "Configuration:"
    echo "  .env file must exist with SUPABASE_URL, SUPABASE_ANON_KEY, etc."
    echo "  Run ./scripts/generate-env.sh to create template."
    echo ""
    echo "Documentation:"
    echo "  spec/docs2code/constitution.md  - Core principles"
    echo "  spec/docs2code/prd.md           - Requirements"
    echo "  spec/docs2code/plan.md          - Architecture"
    echo "  agents/*.SKILL.md               - Agent boundaries"
    echo ""
}

# ==============================================================================
# Main
# ==============================================================================

main() {
    # Ensure we have a command
    if [ $# -eq 0 ]; then
        show_help
        exit 0
    fi

    # Load environment and setup directories
    load_environment
    ensure_directories

    # Route to command
    case "$1" in
        ingest)
            cmd_ingest
            ;;
        build)
            cmd_build
            ;;
        release)
            cmd_release
            ;;
        -h|--help|help)
            show_help
            ;;
        *)
            log_fail "Unknown command: $1"
            show_help
            exit 1
            ;;
    esac
}

main "$@"
